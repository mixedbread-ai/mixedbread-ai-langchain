"""
Example usage of Mixedbread AI Vector Store integration with LangChain.

This example demonstrates:
1. Creating and managing vector stores
2. Uploading files to vector stores
3. Using chunk-based and file-based retrievers
4. Integrating with LangChain workflows
"""

import asyncio
from pathlib import Path

from mixedbread_ai_langchain import (
    MixedbreadVectorStoreManager,
    MixedbreadVectorStoreRetriever,
    MixedbreadVectorStoreFileRetriever,
)


async def main():
    # Initialize the vector store manager
    manager = MixedbreadVectorStoreManager(api_key="your-api-key")

    print("üöÄ Setting up vector store...")

    # Example 1: Create a vector store and upload files
    vector_store_id, file_ids = await manager.asetup_vector_store_with_files(
        name="AI Research Papers",
        file_paths=["paper1.pdf", "paper2.pdf", "research_notes.docx"],
        description="Collection of AI research papers and notes",
    )

    print(f"‚úÖ Created vector store: {vector_store_id}")
    print(f"üìÅ Uploaded {len(file_ids)} files")

    # Example 2: Create a chunk-based retriever
    print("\nüîç Setting up chunk-based retriever...")

    chunk_retriever = MixedbreadVectorStoreRetriever(
        vector_store_ids=[vector_store_id],
        top_k=5,
        score_threshold=0.7,
        return_metadata=True,
    )

    # Search for relevant chunks
    query = "What are the latest developments in transformer architectures?"
    chunk_results = await chunk_retriever.aget_relevant_documents(query)

    print(f"üìÑ Found {len(chunk_results)} relevant chunks:")
    for i, doc in enumerate(chunk_results):
        print(f"  {i+1}. Score: {doc.metadata.get('relevance_score', 0):.3f}")
        print(f"      Source: {doc.metadata.get('source', 'Unknown')}")
        print(f"      Preview: {doc.page_content[:100]}...")
        print()

    # Example 3: Create a file-based retriever
    print("\nüìÅ Setting up file-based retriever...")

    file_retriever = MixedbreadVectorStoreFileRetriever(
        vector_store_ids=[vector_store_id],
        top_k=3,
        include_chunks=True,
        chunk_limit=2,  # Include top 2 chunks per file
        return_metadata=True,
    )

    # Search for relevant files
    file_results = await file_retriever.aget_relevant_documents(query)

    print(f"üìã Found {len(file_results)} relevant files:")
    for i, doc in enumerate(file_results):
        print(f"  {i+1}. Score: {doc.metadata.get('relevance_score', 0):.3f}")
        print(f"      File: {doc.metadata.get('filename', 'Unknown')}")
        print(f"      Chunks included: {doc.metadata.get('chunks_included', 0)}")
        print(f"      File type: {doc.metadata.get('file_type', 'Unknown')}")
        print()

    # Example 4: Using the manager's convenience method to create retrievers
    print("\n‚ö° Using manager convenience methods...")

    # Create retrievers directly through the manager
    quick_chunk_retriever = manager.create_retriever(
        vector_store_ids=[vector_store_id], retriever_type="chunk", top_k=3
    )

    quick_file_retriever = manager.create_retriever(
        vector_store_ids=[vector_store_id], retriever_type="file", include_chunks=False
    )

    # Example 5: Multi-vector store search
    print("\nüîÑ Multi-vector store search example...")

    # Create another vector store for comparison
    second_store_id = await manager.acreate_vector_store(
        "Technical Documentation", "API docs and technical guides"
    )

    # Multi-store retriever
    multi_retriever = MixedbreadVectorStoreRetriever(
        vector_store_ids=[vector_store_id, second_store_id],
        top_k=10,
        score_threshold=0.6,
    )

    # Example 6: Integration with LangChain RAG workflow
    print("\nü§ñ RAG workflow example...")

    def simple_rag_pipeline(query: str, retriever):
        """Simple RAG pipeline example."""
        # Retrieve relevant documents
        docs = retriever.get_relevant_documents(query)

        # Combine retrieved content
        context = "\n\n".join([doc.page_content for doc in docs[:3]])

        # Create prompt (in real use, you'd use an LLM here)
        prompt = f"""
        Based on the following context, answer the question:
        
        Context:
        {context}
        
        Question: {query}
        
        Answer: [This would be generated by your LLM]
        """

        return {
            "query": query,
            "retrieved_docs": len(docs),
            "context_length": len(context),
            "sources": [doc.metadata.get("source") for doc in docs[:3]],
        }

    # Run RAG pipeline
    rag_result = simple_rag_pipeline(query, chunk_retriever)
    print("RAG Pipeline Results:")
    print(f"  Query: {rag_result['query']}")
    print(f"  Retrieved docs: {rag_result['retrieved_docs']}")
    print(f"  Context length: {rag_result['context_length']} chars")
    print(f"  Sources: {rag_result['sources']}")

    # Example 7: Managing vector stores
    print("\nüìä Vector store management...")

    # List all vector stores
    stores = await manager.alist_vector_stores()
    print(f"Total vector stores: {len(stores)}")

    # Get specific store info
    store_info = await manager.aget_vector_store_info(vector_store_id)
    print(f"Store '{store_info.get('name')}' info:")
    print(f"  ID: {store_info.get('id')}")
    print(f"  Description: {store_info.get('description')}")
    print(f"  Created: {store_info.get('created_at', 'Unknown')}")


def sync_example():
    """Synchronous usage example."""
    print("\nüîÑ Synchronous usage example...")

    manager = MixedbreadVectorStoreManager(api_key="your-api-key")

    # Create vector store
    store_id = manager.create_vector_store("Sync Example Store")

    # Upload a file
    # file_id = manager.upload_file(store_id, "example.pdf")

    # Create retriever
    retriever = manager.create_retriever([store_id], retriever_type="chunk")

    # Search
    # results = retriever.get_relevant_documents("example query")

    print(f"‚úÖ Created sync vector store: {store_id}")


if __name__ == "__main__":
    print("üéØ Mixedbread AI Vector Store LangChain Integration Example")
    print("=" * 60)

    # Run async example
    asyncio.run(main())

    # Run sync example
    sync_example()

    print("\n‚ú® Example completed!")
    print("\nNext steps:")
    print("1. Replace 'your-api-key' with your actual Mixedbread AI API key")
    print("2. Add real file paths for upload")
    print("3. Integrate with your favorite LLM for complete RAG pipeline")
    print("4. Explore different search options and configurations")
